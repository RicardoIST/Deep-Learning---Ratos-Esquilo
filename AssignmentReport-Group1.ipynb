{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "\n",
    "As explained in the worksheet, to minimize the cost function with gradient descent, the gradient of the cost function ($C^n(w) = -\\big(y^n \\ln{(\\hat{y}^n)} + (1-y^n) \\ln{(1-\\hat{y^n})}\\big)$)\n",
    "is required, with $\\hat{y}^n$ being the sigmoid activation function  ($\\hat{y}^n = f(x^n) = \\frac{1}{1+ e^{-w^T x^n}}$).\n",
    "\n",
    "Using the chain rule and knowing the fact that $\\frac{\\partial f\\left(x^{n}\\right)}{\\partial w_{i}}=x_{i}^{n} f\\left(x^{n}\\right)\\left(1-f\\left(x^{n}\\right)\\right)$ (equation 7 in the worksheet) it's possible to determine the gradient of the Logistic Regression.\n",
    "\n",
    "From the cross entropy loss equation and making a change of variable $\\hat y^n=f(x^n)$, the gradient is given by:\n",
    "\\begin{align}\n",
    "%\\begin{split}\n",
    "    \\frac{\\partial}{\\partial w_i} C^n(w) &= - \\frac{\\partial}{\\partial w_i}( y^n \\ln{f(x^n)} +  (1-y^n) \\ln{(1-f(x^n))})\\\\\n",
    "    &= - y^n \\Big(\\frac{1}{f(x^n)} x_i^n f(x^n)(1-f(x^n)) - (1-y^n) \\frac{1}{1-f(x^n)}x_{i}^{n} f\\left(x^{n}\\right)\\left(1-f\\left(x^{n}\\right)\\right) \\Big) \\\\\n",
    "    &= - y^n x_i^n (1-f(x^n)) + (1-y^n)x_i^nf(x^n) \\\\\n",
    "    &=-x_i^n(y^n - f(x^n))\n",
    "%\\end{split}\n",
    "\\end{align}\n",
    "Since $\\hat y^n=f(x^n)$, then $\\frac{\\partial}{\\partial w_i} C^n(w)=-x_i^n(y^n - \\hat{y^n})$ q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "In this exercise we seek to obtain an expression of the Softmax regression gradient for a training set, it being defined by the equation (5) of the worksheet given, where $\\hat{y_k^n} = \\frac{e^{z_k^n}}{\\sum_{k'}^K e^{z_{k'}^n}}$, with $z_k^n = w_k^T \\cdot x^n = \\sum_i w_{k,i}\\cdot x_i^n$.\n",
    "\n",
    "In order to make calculations easier we can, once again, simplify the expression given like we can see beneath:\n",
    "\n",
    "\\begin{align}\n",
    "    C^n(w) &= -\\sum_{k=1}^K y_k^n \\ln{\\hat{y_k^n}} \\\\ \n",
    "    &= -\\sum_{k=1}^K y_k^n \\ln{\\bigg(\\frac{e^{z_k^n}}{\\sum_{k'}^K e^{z_{k'}^n}}\\bigg)} \\\\\n",
    "    &= -\\sum_{k=1}^K y_k^n \\Bigg( \\ln{(e^{z_k^n})} - \\ln{\\bigg(\\sum_{k'}^K e^{z_{k'}^n}\\bigg)} \\Bigg)  \\\\\n",
    "    &= -\\sum_{k=1}^K y_k^n z_k^n +\\sum_{k=1}^K y_k^n \\ln{\\bigg(\\sum_{k'}^Ke^{z_{k'}^n}\\bigg)} \\\\\n",
    "    &= -\\sum_{k=1}^K y_k^n z_k^n +  \\ln{\\bigg(\\sum_{k'}^Ke^{z_{k'}^n}\\bigg)}= -\\alpha^n(w) + \\beta^n(w)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Differentiating $\\alpha(w)$ gives\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial w_{kj}} \\alpha(w) &= \\frac{\\partial}{\\partial w_{kj}} \\sum_{k=1}^K y_k z_k \\\\\n",
    "    &= \\frac{\\partial}{\\partial w_{kj}} \\sum_{k=1}^K y_k \\sum_{i=1}^{785} w_{ki}x_i \n",
    "\\end{align}\n",
    "\n",
    "By developing some terms of the sums we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial w_{kj}} g(w) = \\frac{\\partial}{\\partial w_{kj}}\\Big(y_1(w_{11}x_1 + w_{12}x_2 + \\dots + w_{1i}x_i)\n",
    "    +  y_2(w_{21}x_1 + w_{22}x_2 + \\dots + w_{2i}x_i)\n",
    "    +  y_k(w_{k1}x_1 + w_{k2}x_2 + \\dots + w_{ki}x_i)  \\Big)\n",
    "\\end{equation}\n",
    "\n",
    "We can then assume that the differentiation of any term from the expressions above is different from zero only if $i=j$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial w_{kj}} \\alpha(w) = y_k \\cdot x_j\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Differentiating $\\beta(w)$ gives\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial }{\\partial w_{kj}} \\beta(w) &= \\frac{\\partial }{\\partial w_{kj}} \\ln{\\bigg(\\sum_{k'}^Ke^{z_{k'}}\\bigg)} \n",
    "    = \\frac{\\partial }{\\partial w_{kj}} \\ln{\\bigg(\\sum_{k'}^Ke^{\\sum_{i} w_{k'i} \\cdot x_i}\\bigg)}  \n",
    "    = \\frac{1}{\\sum_{k'}^Ke^{\\sum_{i} w_{k'i} \\cdot x_i}} \\frac{\\partial }{\\partial w_{kj}} \\sum_{k'}^Ke^{\\sum_{i} w_{k'i} \\cdot x_i} \n",
    "\\end{align}\n",
    "\n",
    "By writing out the sums of the remaining expression to be differentiated in the equation above we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial }{\\partial w_{kj}} \\sum_{k'}^Ke^{\\sum_{i} w_{k'i} \\cdot x_i} =  \\frac{\\partial }{\\partial w_{kj}} \\bigg(  e^{\\sum_i w_{1i}x_i} + e^{\\sum_i w_{2i}x_i} + \\dots + e^{\\sum_i w_{Ki}x_i}\\bigg)=\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    = \\Big(\\frac{\\partial }{\\partial w_{kj}} \\sum_i w_{1i}x_i \\Big)e^{\\sum_i w_{1i}x_i} \n",
    "     + \\Big(\\frac{\\partial }{\\partial w_{kj}} \\sum_i w_{2i}x_i \\Big)e^{\\sum_i w_{2i}x_i}\n",
    "      + \\dots + \\Big(\\frac{\\partial }{\\partial w_{kj}} \\sum_i w_{Ki}x_i \\Big)e^{\\sum_i w_{Ki}x_i} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Once again the derivative mentioned above is only different from zero when $j=i$ and $k = k'$, with all the non-zero derivatives being defined by $x_j e^{\\sum_i w_{ki}x_i}$. We then get the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial }{\\partial w_{kj}} \\sum_{k'}^Ke^{\\sum_{i} w_{k'i} \\cdot x_i} = x_j e^{\\sum_i w_{ki}x_i}\n",
    "\\end{equation}\n",
    "\n",
    "Combining EQUACAO X into EQUAÇÃO Y we get:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial }{\\partial w_{kj}} \\alpha(w) &= \\frac{1}{\\sum_{k'}^Ke^{\\sum_{i} w_{k'i} \\cdot x_i}} x_j e^{\\sum_i w_{ki}x_i} \\\\\n",
    "    &= \\frac{e^{z_k}}{\\sum_{k'}^K e^{z_{k'}}} x_j  \\\\\n",
    "    &= \\hat{y_k}x_j \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Ultimately, by gathering the results of both EQUACAO A and EQUAÇÃO B we get to the expression of the Softmax regression gradient for a training set we've been trying to obtain.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "The results of the loss function can be seen in the figure above.\n",
    "It can be seen that the model will converge relatively fast, with the validation loss showing better results when compared to the training loss.\n",
    "The only problem is the spikes that can be seen in the plot.\n",
    "\n",
    "\n",
    "![](task2b_binary_train_loss.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "This graph ilustrate the function that computes the accuracy of the training set and the validation set.\n",
    "\n",
    "As it can be seen, the accuracy of both sets converge around of 0.98 (or 98%)\n",
    "\n",
    "![](task2b_binary_train_accuracy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The Early Stopping is used to reduce the overfit on the training dataset by stopping the train.\n",
    "By noticing that the cost function on the validation set stops to improve it is possible to prevent overfit by stop the training and return the weights that corresponds to the best validation loss.\n",
    "\n",
    "In our implementation the early stopping will occur if the loss doesn't decrease after passing 20% of training data 10 times.\n",
    "\n",
    "The training stopped after the ephoc numeber 14 out of 50.\n",
    "\n",
    "With the early stopping the performance metrics in the task2 are:\n",
    "\n",
    "-Final Train Cross Entropy Loss: 0.08958487019189541\n",
    "\n",
    "-Final Validation Cross Entropy Loss: 0.08835086674263219\n",
    "\n",
    "-Train accuracy: 0.9688103781396633\n",
    "\n",
    "-Validation accuracy: 0.9765258215962441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "By shuffling the dataset between epochs the network convergence can be improved.\n",
    "\n",
    "The shuffling is done to make sure that the network doesn't encounter the same examples in each epoch and so prevents the memorization of the training set. In reallity what it's wanted is to create a model that describes the data because if the model memorize the data will result in overfitting and in consequence bad results in the test sets.\n",
    "\n",
    "Its possible to observe that the training set with shuffle has less spikes in both plots below. This occur because the spikes represent bathes that are extrimely difficult to anallyse and so, by shuffling the data it's possible to reduce their effect and so the spikes are smaller.\n",
    "\n",
    "\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)\n",
    "\n",
    "\n",
    "![](task2e_train_loss_with_shuffle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "After putting into effect the desired functionality (Softmax regression with mini-batch gradient descent for a single layer neural network), we followed the training loss for every gradient step. Additionally we decided to resort to cross-vallidation with 20% of the training test (and therefore using the other 80% to train the model). The loss results are plotted beneath and they clearly confirm what we already knew from theory, a decrease in loss for the validation set with each iteration, tending to always be smaller than the training loss (as expected).\n",
    "\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "After putting into effect the desired functionality (a function that computes the multi-class classification accuracy over a dataset) and plotting the resulting accuracy beneath, we can clearly notice that after approximately XXXX sets, the vallidation accuracy doesn't increase as much as the training one does. This last conclusion makes sense since the training accuracy is always supposed to be higher than the validation one, this happens because of the data used to train the model (training set, opposed to the validation set).\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "With the training of our model there are two situations that must be avoided, overfitting and underfitting. And what are overfitting and underfitting? When a model gets excessively data-sensitive, it captures a lot of noise and random patterns that don't adapt well to new data. This is known as overfitting. While a model like this normally performs well on the training set, it struggles on the test set. Underfitting on the other hand, is a problem that occurs when a model fails to capture enough patterns in a dataset, resulting in poor performance in both the training and test sets.\n",
    "\n",
    "By analyzing the plot shown above (figure X), it's trivial that the accuracy regarding the training set is much better than the validation one especially after around XXXX training steps, and this is a clear indicator of overfitting, and it would be even more notorious if the training continued for even more training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a) Softmax gradient with L2 regularization\n",
    "\n",
    "We want to obtain the new parcel for the Softmax Regression after inserting L2 regression, given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    J(w) = C(w) + \\lambda R(w) \\enskip , \\enskip \\lambda \\in \\mathbb{R}\n",
    "\\end{equation} \n",
    "where $C(w)$ is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    C(w) = \\frac{1}{N}\\sum_{n=1}^N C^n(w)\n",
    "\\end{equation}\n",
    "\n",
    "with $C^n(w)$ being the  expression of the Softmax regression gradient for a training set defined above. \n",
    "\n",
    "$R(w)$ is defined beneath as the square of the $L_2$ norm of the weight matrix, $w$.\n",
    "\n",
    "\\begin{equation}\n",
    "    R(w) = \\|w\\|^2 = \\sum_{i,j} w_{ij}^2\n",
    "\\end{equation}\n",
    "\n",
    "The new parcel is given by the partial derivative of J with respect to $w$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial w} = \\frac{\\partial C(w)}{\\partial w} + \\lambda \\frac{\\partial R(w)}{\\partial w}\n",
    "\\end{equation}\n",
    "\n",
    "Once again, since $J(w)$ consists on a sum of two terms, it's easier to take the derivative of it by differentiating both terms separately. \n",
    "\n",
    "By differentiating $C(w)$ we get:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial C(w)}{\\partial w} &= \\frac{\\partial}{\\partial w} \\frac{1}{N} \n",
    "    \\sum_{n=1}^N C^n(w) \\\\\n",
    "    &= \\frac{1}{N} \\sum_{n=1}^N \\frac{\\partial}{\\partial w} C^n(w) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Using the results we obtained from task 1b, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(w)}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^N \\frac{\\partial}{\\partial w}C^n(w)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "By differentiating $R(w)$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial R(w)}{\\partial w} = \\frac{\\partial \\|w\\|^2}{\\partial w} = \\frac{\\partial}{\\partial w} \\sum_{i,j} w_{i,j}^2 \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Considering the matrix gotten above identical to the one we obtained in both tasks 1a and 1b, the derivative for a given matrix entry, $\\frac{\\partial}{\\partial w_{i',j'}} \\sum_{i,j}w_{i,j}^2$ is only different from zero when $i' = i$ and $j'=j$, these matrix entries being defined by  $2w_{i',j'}$. Therefore we can infer that the derivative of $R(w)$ is equal to $2w$, with $w$ being the weight matrix.\n",
    "\n",
    "Finally by adding the results of both derivatives taken, we get to the new updated term for the Softmax regression with $L_2$ regularization:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^N \\frac{\\partial}{\\partial w}C^n(w) + 2\\lambda w\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c / 4d)\n",
    "In the plot bellow it can be observed the training and validation accuracy for the model with different values of lambda   $\\lambda$ ={0.002,0.02.0.2,2}.\n",
    "\n",
    "It can be observed that when $\\lambda$ increases the accuracy will decrease. This happen because when regularization makes the model simpler, howerver, if the weights are too small for the classification task, the model will become too simple and in result the results will be much poor as the regularization increases.\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "As it can be seen the the plot below, with the increasing of $\\lambda$ the model gets more simple and in consequence the ||w|| (norm of w) gets smaller.\n",
    "\n",
    "In this particular case it can be seen that the results aren't good.\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
